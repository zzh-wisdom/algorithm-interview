# LRU-Cache

<https://zhuanlan.zhihu.com/p/34218460>
<https://juejin.cn/post/6844904049263771662>

依据时间局部性原理及系统资源限制，**cache使用的资源不用无限大**，要设定最大使用资源限制，这就涉及到当cache消耗达到最大资源，旧数据如何退出（老化）？

LRU 全名指最近最少使用算法，思想是**最近被使用的数据将来被访问的概率也更高**，所以数据老化的时候老化掉『最远』被使用数据

很多人可能会想到保持一个排序队列，当有新数据项写入或者数据项被读取的时候，调整排序队列，但是这样每次的效率都是O(N)，如何做到O(1)呢？

## 实现思路

常见实现思路：

**map+双向链表**

实现O(1)的插入查询

缺点：因为操作是互斥的，所以这2个数据结构都需要加锁

## 性能优化思路

1. **锁粒度**优化：比如hash表的锁，可以对每个hash桶对应的冲突链进行加锁，类似Leveldb的做法，使用分片Cache，将key通过hash函数，映射到不同的分片上，然后再对该分片进行加锁，**减少锁粒度，提高并发度**。
2. **延迟淘汰**：如上图，当超过一定资源的时候，要对链表末尾淘汰数据，这本身就有一次加锁操作，可以进行延迟淘汰，当积累的要淘汰的数据积累一些后再淘汰，操作办法：如果要删掉末尾数据，可以只把链表末尾指针前移，不需要进行链表node删除，这个操作是原子的，**等积累一定待删除node后（看资源设定个数阀门）再删除**。（对象析构可能会非常耗时，现在的问题是每次访问LRUCache就要遭遇它，办法也就有了：1）一次淘汰一批元素（比如最后10个？），2）发起独立线程专门处理元素删除。）
3. cache中保存的对象肯定是创建比较频繁的，**所以复用而不是删除后再创建应该是个好想法，也就是把要淘汰的元素放到相应对象池，下次创建时直接复用它们**！完善的基础库应该有对象池(或者内存池)。
4. **命中率优化**：**多层cache** 是可以提升cache命中率的，比如在搜索系统中测试发现，2层lru cache的命中率比单层cache能提升5%。如**LRU-K算法**，防止偶然的批处理对cache的污染，本质上是多级cache
5. thread local，**每个线程一个独立的cache**，但效果和减少锁粒度的比较起来，还需要实际测试。但并发度肯定大大提高，命中率降低

**总结**：最极致的Cache还是服务化，专用化，为不同的负载场景使用不同的cache

## 实现细节

**无拷贝**：要想无拷贝最简单的就是用裸指针，拷贝它相对廉价的多，但可惜不行，因为LRUCache有淘汰策略，如果在删除对象时另一个线程还在引用它，那就完了。我们不由想到了shared_ptr，就是它了！它比裸指针多出了原子变量加减计数的耗时。就是**引用计数**

**降低元素移动频率**：LRUCache总是将最近访问的元素放到list头部，例如cache命中时。如果LRUCache实现是用的双向list，这个操作下来需要6次指针拷贝，包括修改list的指针和map。一种“优化”思路是：在list的每个元素里定义一个计数变量，**记录访问次数，当cache命中时加1；只有当命中时发现这个数字大于一个阈值后才把它移动到list头**；移动后将记录清零，从头开始。这个思路看上去能大幅减少指针操作次数，所以应该是一种“优化”，不过这当然要以实际的命中率为准。**命中率会降低，毕竟淘汰不是严格的LRU，实际效果还是要测试**。

## 衍生问题及解决思路

问题：当热点数据较多时，有较高的命中率，但是如果有**偶发性的批量操作**，会使得热点数据被非热点数据挤出容器，使得缓存受到了“污染”。所以为了消除这种影响，又衍生出了下面这些优化方法。

**LRU-K**

LRU-K算法是对LRU算法的改进，将原先进入缓存队列的评判标准从访问一次改为访问K次，可以说朴素的LRU算法为LRU-1。

LRU-K算法有两个队列，一个是缓存队列，一个是数据访问历史队列。当访问一个数据时，首先先在访问历史队列中累加访问次数，**当历史访问记录超过K次后，才将数据缓存至缓存队列，从而避免缓存队列被污染**。同时访问历史队列中的数据可以按照LRU的规则进行淘汰。具体如下图所示：

综合来说，使用**LRU-2**的性能最优。

实现思路：

历史记录和缓存记录都分别用一个LRUCache来维护。不同的是历史记录中，LRUCache的value为key的访问次数。
1）读操作：历史记录中，将该key的访问次数加一， historyList.put(key, ++historyCount);然后直接返回从缓存记录中的Get结果
2）Put，先判断缓存记录中是否包含，是直接返回；否则，从历史记录中判断该key的历史访问次数是否大于等于k，是则将它从历史记录中删除，然后加入缓存记录中。

可见，历史记录和缓存记录中的key存在重合，为了更好的性能，应该设置历史记录的大小大于缓存记录大小，比如设置为两倍。

上面是为了简单起见，在多级cache中，这样实现是不合理的，应让每一级的cache的key不重复，将上一级cache淘汰的元素再插入本级cache。

实际上就是多级cache，不同的是，后面的cache只记录key的访问次数，没有value。

## Multi Queue

相比于上面两种优化，Multi Queue的实现则复杂的多，顾名思义，Multi Queue是由多个LRU队列组成的。每一个LRU队列都有一个相应的优先级，数据会根据访问次数计算出相应的优先级，并放在该队列中。

1. 数据插入和访问：当数据首次插入时，会放入到优先级最低的Q0队列。当再次访问时，根据LRU的规则，会移至队列头部。当根据访问次数计算的优先级提升后，会将该数据移至更高优先级的队列的头部，并删除原队列的该数据。同样的，当该数据的优先级降低时，会移至低优先级的队列中。

2. 数据淘汰：数据淘汰总是从最低优先级的队列的末尾数据进行，并将它加入到Q-history队列的头部。如果数据在Q-history数据中被访问，则重新计算该数据的优先级，并将它加入到相应优先级的队列中。否则就是按照LRU算法完全淘汰。

